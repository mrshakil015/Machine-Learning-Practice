# Machine Learning Basic
<details>
  <summary><b> What is batch_size?</b></summary>
    - Batch size refers to the number of samples or data points that a machine learning algorithm uses in one iteration or training step. In other words, it determines how many examples are processed at once by the algorithm during training. For instance, if a dataset contains 1000 training examples, and the batch size is set to 32, the algorithm would take 32 examples at a time and update the weights of the model accordingly. The process of updating the weights after processing each batch of data is called stochastic gradient descent (SGD).The batch size can affect the accuracy and speed of the training process. A larger batch size can speed up the training process, but it can also cause the model to generalize poorly. A smaller batch size can lead to slower training times but may improve the accuracy of the model. Choosing the appropriate batch size is a trade-off between these factors and depends on the specific problem being addressed.
</details>


